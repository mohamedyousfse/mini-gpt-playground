{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yv4rOc5SMhw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üî• Mini GPT Playground ‚Äì By Mohamed yousef\n",
        "\n",
        "# ‚úÖ 1. Self-Attention Visualization\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "attention_scores = np.array([\n",
        "    [0.2, 0.3, 0.1, 0.1, 0.2, 0.1],\n",
        "    [0.1, 0.4, 0.2, 0.1, 0.1, 0.1],\n",
        "    [0.1, 0.2, 0.4, 0.1, 0.1, 0.1],\n",
        "    [0.1, 0.1, 0.1, 0.4, 0.2, 0.1],\n",
        "    [0.2, 0.1, 0.1, 0.2, 0.3, 0.1],\n",
        "    [0.1, 0.1, 0.1, 0.1, 0.2, 0.4],\n",
        "])\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(attention_scores, xticklabels=tokens, yticklabels=tokens, annot=True, cmap='Blues')\n",
        "plt.title(\"Self-Attention Visualization\")\n",
        "plt.xlabel(\"Focus On\")\n",
        "plt.ylabel(\"Source Word\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tfvBtpuIWNMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ 2. Manual Self-Attention (3 Words)\n",
        "\n",
        "tokens = [\"I\", \"love\", \"AI\"]\n",
        "X = np.array([\n",
        "    [1.0, 0.0, 1.0],\n",
        "    [0.5, 1.0, 0.5],\n",
        "    [1.0, 1.0, 0.0]\n",
        "])\n",
        "\n",
        "W_q = np.random.rand(3, 3)\n",
        "W_k = np.random.rand(3, 3)\n",
        "W_v = np.random.rand(3, 3)\n",
        "\n",
        "Q = X @ W_q\n",
        "K = X @ W_k\n",
        "V = X @ W_v\n",
        "\n",
        "scores = Q @ K.T\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
        "\n",
        "attention_weights = softmax(scores)\n",
        "attention_output = attention_weights @ V\n",
        "\n",
        "print(\"üîç Attention Weights:\")\n",
        "print(attention_weights)\n",
        "print(\"\\nüß† Final Output after Attention:\")\n",
        "print(attention_output)\n"
      ],
      "metadata": {
        "id": "TGxnMeNLXIFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ 3. Multi-Head Attention (2 Heads)\n",
        "\n",
        "num_heads = 2\n",
        "head_dim = 3\n",
        "all_heads_output = []\n",
        "\n",
        "for head in range(num_heads):\n",
        "    W_q = np.random.rand(3, head_dim)\n",
        "    W_k = np.random.rand(3, head_dim)\n",
        "    W_v = np.random.rand(3, head_dim)\n",
        "\n",
        "    Q = X @ W_q\n",
        "    K = X @ W_k\n",
        "    V = X @ W_v\n",
        "\n",
        "    scores = Q @ K.T\n",
        "    attn = softmax(scores)\n",
        "    out = attn @ V\n",
        "    all_heads_output.append(out)\n",
        "\n",
        "    print(f\"\\nüîÅ Head {head + 1} - Attention Weights:\")\n",
        "    print(attn)\n",
        "\n",
        "# Combine the outputs of all heads\n",
        "multi_head_output = np.concatenate(all_heads_output, axis=-1)\n",
        "\n",
        "print(\"\\nüß† Final Multi-Head Output:\")\n",
        "print(multi_head_output)\n"
      ],
      "metadata": {
        "id": "nBq8nIVDXOG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ 4. Mini Text Generator (GPT-style logic)\n",
        "\n",
        "import random\n",
        "\n",
        "transitions = {\n",
        "    \"The\": [\"sky\", \"sun\"],\n",
        "    \"sky\": [\"is\"],\n",
        "    \"sun\": [\"shines\"],\n",
        "    \"is\": [\"blue\"],\n",
        "    \"shines\": [\"bright\"],\n",
        "    \"blue\": [\"and\"],\n",
        "    \"and\": [\"the\"],\n",
        "    \"the\": [\"sun\"],\n",
        "}\n",
        "\n",
        "def generate_sentence(start_word=\"The\", max_len=10):\n",
        "    sentence = [start_word]\n",
        "    current = start_word\n",
        "    for _ in range(max_len - 1):\n",
        "        next_words = transitions.get(current)\n",
        "        if not next_words:\n",
        "            break\n",
        "        current = random.choice(next_words)\n",
        "        sentence.append(current)\n",
        "    return \" \".join(sentence)\n",
        "\n",
        "print(\"\\nüìù Generated Sentences:\")\n",
        "for _ in range(3):\n",
        "    print(generate_sentence())\n"
      ],
      "metadata": {
        "id": "XJIwsm1nXUdj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}